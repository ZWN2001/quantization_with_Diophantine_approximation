{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下载模型\n",
    "\n",
    "huggingface-cli download --resume-download charent/Phi2-Chinese-0.2B --local-dir Phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from waitress import serve\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "import multiprocessing\n",
    "from functools import partial\n",
    "from fractions import Fraction\n",
    "import utils_function\n",
    "# import dill\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义模型路径\n",
    "local_model_path = \"./Phi\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载分词器和模型\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(local_model_path, trust_remote_code=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        local_model_path,\n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=torch.float32,\n",
    "        device_map='auto'\n",
    "    )\n",
    "    print(\"Model and tokenizer loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading or saving model: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提取权重\n",
    "print(\"Extracting model weights...\")\n",
    "state_dict = model.state_dict()  # 获取模型的权重\n",
    "print(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict_numpy = {key: value.cpu().numpy() for key, value in state_dict.items()}\n",
    "\n",
    "quantized_weights_map = {}\n",
    "quantized_float_weights_map = {}\n",
    "\n",
    "# for key, value in state_dict_numpy.items():\n",
    "#     print(f\"层名称: {key}\")\n",
    "#     print(f\"原始权重矩阵:\\n{value}\")\n",
    "#     quantized_weights = utils_function.approximate_with_rational(value, precision = 2)\n",
    "#     print(f\"量化后的权重矩阵:\\n{quantized_weights}\")\n",
    "#     quantized_weights_map[key] = quantized_weights\n",
    "#     float_weights = utils_function.convert_fractions_to_floats(quantized_weights,precision = 4)\n",
    "#     print(f\"转换为浮点数的权重矩阵:\\n{float_weights}\")\n",
    "#     quantized_float_weights_map[key] = float_weights\n",
    "\n",
    "def process_layer(key, value, precision=2, float_precision=4):\n",
    "    # 量化权重矩阵\n",
    "    quantized_weights = utils_function.approximate_with_rational(value, precision=precision)\n",
    "    # 转换为浮点数的权重矩阵\n",
    "    float_weights = utils_function.convert_fractions_to_floats(quantized_weights, precision=float_precision)\n",
    "    return key, quantized_weights, float_weights\n",
    "\n",
    "\n",
    "def process():\n",
    "    # 定义量化精度\n",
    "    precision = 2\n",
    "    float_precision = 4\n",
    "\n",
    "    # 创建部分函数，固定精度参数\n",
    "    process_layer_fn = partial(process_layer, precision=precision, float_precision=float_precision)\n",
    "\n",
    "    # 将 state_dict_numpy 的 items 转换为列表，用于并行处理\n",
    "    items = list(state_dict_numpy.items())\n",
    "\n",
    "    # 初始化多进程池\n",
    "    with multiprocessing.Pool(processes=multiprocessing.cpu_count()) as pool:\n",
    "        results = pool.starmap(process_layer_fn, items)\n",
    "\n",
    "    # 解析结果\n",
    "    quantized_weights_map = {key: quantized_weights for key, quantized_weights, _ in results}\n",
    "    quantized_float_weights_map = {key: float_weights for key, _, float_weights in results}\n",
    "\n",
    "    # 返回或打印最终结果\n",
    "    return quantized_weights_map, quantized_float_weights_map\n",
    "\n",
    "quantized_weights_map, quantized_float_weights_map = process()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_state_dict = OrderedDict()\n",
    "\n",
    "for key,value in quantized_float_weights_map.items():\n",
    "    converted_state_dict[key] = torch.tensor(value, device='cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(converted_state_dict)\n",
    "torch.save(converted_state_dict, \"model_weights.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(converted_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"讲讲快排算法的原理和实现过程。\"\n",
    "\n",
    "messages = [{'role': 'user', 'content': question}]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    chat_template=\"content\",  \n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "\n",
    "outputs = model.generate(\n",
    "    inputs,\n",
    "    max_new_tokens=1024,\n",
    "    do_sample=False,\n",
    "    top_k=50,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "response = tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
